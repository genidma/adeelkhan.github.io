# Newer realities

Okay, in one of my previous blogposts, I said that, [quote](https://stellardreams.github.io/Ethics-involving-Terraforming/) I am not a pro A.I fellow or an anti A.I fellow. 

* First of all, classification of intelligence as artificial doesn't seem apt. The way I see it, our intelligence has been enabled because eukaryotic cells figured out how to co-operate over a very very longer stretch of time. Out of this co-operation, came pretty much all of the life that we see around us. Much of which has yet to be discovered and catalogued. Plus, all of the life we see around ourselves is only a tiny fraction of the life that has existed on this planet.  Plus, if we think about it, the species that are really thriving on this planet are actually bacteria and viruses. Left to their own, they'd be exposed to radiation from the sun and heat from all these different sources. These conditions would result into significantly shortening their lifespan and making for rather unpleasant living conditions. So maybe the cells are a lot more clever. First of all, biology has kept chains going for a very very long time. Secondly by creating complex structure that the cells can reside in, they've significantly increased their own lifespan - as well the quality of their life. This is not an original thought entirely. I have heard Dr. Martine Rothblatt say something along these lines and also Naveen Jain. 
* And so, I think it makes more sense to categorize intelligence that we are going to model-engineer, as being substrate independent. There is also a moral and ethical angle and as it relates to the classification of intelligence. Because if we classify something as artificial and that results into the automatic classification of that intelligence as non-conscious and it turns out that the intelligence is actually conscious. Then we have enabled suffering by design. However the issue isn't necessarily as clear cut. And between Stephen Hawking, Daniel Dennett, Ray Kurzweil and others, there are different schools of thought on how this substrate independent intelligence should actually be powered. Or if it should be powered at all. 
* Humans go crazy. What is the guarantee that an intelligence that is substrate independent is not going to go crazy. Whether because of a breakdown and as it relates to it's architecture. Or because someone deliberately wanted it to go crazy. For whatever given purpose.
* Next, there is data that biology has powered and humans are also creating tons and tons of data of different kinds. Some of which could actually be useful. It's not possible to mobilize an army of humans and get them to make sense out of this data. An architecture that is engineered seems like the ideal candidate for this purpose. I do not know a lot about machine learning. But basically you construct your data and the way you construct your data is what determines the machine's ability to be able to make sense out of what you have architected. 
* So in a potential hypothethical situation in the very near future, we will need breakthoughs in specific domains and it turns out that the multi-decade long research that we have been doing across the symbolic school and now the connections school is actually quite useful. These models are already beginning to showcase how effective they are. 
* Basically the fear is skynet. Let's think about this. So this is powered by the limbic part of the brain. This fear is expressed and usually there isn't a follow-up. Because machines are going to take over and kill everyone. 
* When I reason and I think I am trying to make sense out of this using first principles. The questions I ask myself are. a) First of all, why would an intelligence that we have engineered, want to serve a gazillion different requests from 10 billion different humans - unless it was specifically designed to do just that. b) How will this entity, how will this being evolve? c) Is it conscious and is it suffering if it is conscious d) Will it go crazy serving hundreds of billions of request each week? e) What if it keeps evolving and we rely on a distributed cluster of these beings for our needs and then it goes caput. Then cluster dies. We revive it back, it functions for a bit and then dies again. Like an electric grid that caves in and the transformers blow out -  when there is too much load. No affliction to OpenAI. I think they are doing a pretty awesome job and from what very limited I gather about them. 


Next and going back to that bit from my previous blogposst, what I meant by that is that the ethical and actual implications of getting breakthroughs and as it relates to us taking steps towards artificial general intelligence (substrate independent intelligence really) is something that we should be mindful of. 
* I am worried that the structure of power dynamics and the way that it has historically evolved for our species, has been adversarial. 
* There doesn't appear to be a clear path towards enabling co-operation. On the contrary, things seem to be moving in the opposite direction. We are seeing some wonderful and remarkable co-operation on the international space station. [Proof](https://blogs.nasa.gov/spacestation/2020/05/). On the surface, there are a lot more of us and we are breaking into each others systems, stealing intellectual property. Our leaders are threatening other leaders with trade wars. They are saying mean and hurtful things about each other. It's just a lot of problems. 
* I think that it's ridiculous to expect the government to solve all the problems. Specifically in a situation whereby a crisis is escalating. There is a threshold of how much stress a system can handle. If we exceed that threshold, then contingen upon the level of severity - we do not know what the outcome is going to be. 
* It's also important to note that different individuals have a different value system.
* If we revive our economies using older models, then we will get the same outcomes. This is something that I expand upon in the [following blogpost](https://stellardreams.github.io/Dealing-with-Outbreaks/). I think this is in the domain of deductive reasoning. But it seems logical and rational to assume that the same set of inputs patterns repeated, are going to yield a similar - if no a far worse outcome. It's highly unlikely that things are going to get better with the same set of inputs. Unless, there are interventions that are incentivized to specifically tackle the problems that arise. And then there are the unintended consequences. 
* On the other hand, enabling newer set of realities will require a fair bit of creative destruction. Being very mindful of the reality that how we have done things in the past, isn't how we are going to be doing things in the future. So we'd need a) Social safety nets. b) Sustainable means to be able to power these social safety nets on a timeline (50 to 100 years at the very least) c) Adequate succession planning and scenario planning. Just handling the algorithms over and training the next generation about the general understanding of the constructs is not going to be enough.
* Now the previous assumption about death/dying may turn out to be not true. Because we may tacklge ageing and disease. In such a scenario, how can we guarantee that newer ideas are going to surface. Also, beyond a certain actual age, should the person continue to reside in a finite geographic space and what are going to be the ethical and legal set of clauses that are going to govern such rulings. 
* Co-operation on a global basis is going to be key. Because the very technologies that we'd need to deploy in order to safeguard our collective future and the future of all the other species that share this planet with us. These very technoogies could indeed be weaponized and turned on us. One of the many many examples, we may have or possibly have (I haven't seen it with my own eyes) the theoretical capability of sending 100 kg payloads to Mars in as little as 3 days. But the same tech could also be abused in order to weaponize space. A potential scenario, that will be to our detriment then and in the future. When we may actually need such weaponry to blast off bigger set of rocks that we may not be able to disintegrate with the existing state of technology that we have. 
* I think the issues are going to get compounded. Because we may deal with further effects of climate change. I am not a climate scientist. So not sure how unpredictable the climate is going to get and when. 
* Also, I think automation is going to displace more jobs vs our ability to be able to create new, quality, well-paying jobs. And even if by some miracle, we manage to create hundreds of millions of quality jobs, then there is going to be a net regressive impact on the ecology. 
* If we go to other worlds in our own solar system and we find evidence of life there, then it's a huge ethical dilemna. Say, push comes to shove and we say, screw it. It's just bacteria and some shrubs and some somewhat intelligent octopuses. Even in such a scenario, we can do some back of the envelope calculations in order to determine when we are going to max out the capacity within our own solar system. I guess, by that time either be or would be on the path to enable structures, that would help us become a [type ii civilization on the Kardashev scale](https://en.wikipedia.org/wiki/Kardashev_scale). However, if we do end up wiping up bacteria and other life-forms on our path, then it's the same ethical dilemna. 

Overall, it seems like safe and effective intelligence that has been engineered is going to be vital in order to build a different reality. 

Not only is it extremely counter-productive to power growth making use of what we have done in the past. But it actually sounds like a really terrible idea. I have shared my reasoning why in previous blogposts. However, here is an analogy that I think is apt (below).

With the whole pandemic situation, I think there is a correlation with regards to all the anti-bacterial sprays and soaps that we use and forest fires. The bigger set of forest fires happen, because we put out the smaller set of fires. The bush, twigs, leaves and what not accumulate and then the whole thing lights up in a bigger blaze. Same with bacteria. We kill all the good ones with the bad ones. Probably over-prescribe too many anti-biotics. So when something like coronavirus does emerge, the impact is significant. To say the least. 

There appear to be other problems in the forms of inputs that are contributing to the problem. 

So, as the cycles of creative destruction happen. It is not inconceivable that a lot of people are going to be angry about the significant changes to their lifestyles. Something that will occur in a somewhat shorter time window. 

The automated factories of the future that [Professor. Nills J. Nillsson](http://ai.stanford.edu/~nilsson/OnlinePubs-Nils/General%20Essays/AIMag05-02-002.pdf) had referred to, is something that has to happen. He shares a timeline of how long it could take to completely automate the entire supply chain system. Perhaps we are already on our way. Howver, it would not be prudent to leave this to chance. Even in the best case scenario, whereby radical abundance would have been powered for all the inhabitants in a completely safe and ecologically friendly way. Even in such a scenario, there could be people who are going to be angry and so it could very well be like [1811](https://en.wikipedia.org/wiki/Luddite) again. But instead of sending in the army and putting down the violence with the justified use of force. It would be much better if we thought about these problems in earnest and started preparing for them now. The ethical and humane thing to do, is to engage with a wider segment of the society and ask them how would they like to engage their minds and their bodies, in a reality where there isn't a necessity to attach work with the means and ability to be able to earn an income. Next, we would then need to work towards designing this reality whereby intelligent life is going to have many healthy avenues of exercising their bodies and keeping their minds engaged. This work, coupled with actually enabling the automated factories of the future will be tackled in a distributed manner. 

All of these are very linear definitions. 

One of the key correlates for getting to breakthroughs is someone who has been wrangling with a given set of conditions in their mind. And so, these problems and more are going to redefined by others. And in the process of doing so, we are going to come up with potential solutions to some of these very problems. 

I think it's a healthy thing to do. To thing about the different set of outcomes and how they could come about. What are the inputs that lead to certain conditions. What are the triggers and where do the necessary support structures exist. 

I'd appreciate it if you could give me feedback on how these blog-posts can be improved. As well, feedback on the how I should structure the processes that lead to constructing these thoughts in the first place. I think I need to take a more structure approach towards describing in chuks, what the proposition actually is. Then go about classifying and describing each one of the sections and then tie is all together.
